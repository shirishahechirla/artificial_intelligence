{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name: Shirisha Hechirla"
      ],
      "metadata": {
        "id": "jbXFDHQCev9-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY4SK0xKAJgm"
      },
      "source": [
        "# RNN Classifier with LSTM Trained on Own Dataset (IMDB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc6xejhY-NzZ"
      },
      "source": [
        "Example notebook showing how to use an own CSV text dataset for training a simple RNN for sentiment classification (here: a binary classification problem with two labels, positive and negative) using LSTM (Long Short Term Memory) cells."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.0.1 torchvision==0.15.2 torchtext==0.15.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppfCVucMRkDl",
        "outputId": "8cce71f5-8540-4b72-821e-bd940228da17"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: torchvision==0.15.2 in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: torchtext==0.15.2 in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (11.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (4.66.6)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.2.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "moNmVfuvnImW"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSRL42Qgy8I8"
      },
      "source": [
        "## General Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "OvW1RgfepCBq"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 123\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "VOCABULARY_SIZE = 20000\n",
        "LEARNING_RATE = 0.005\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 15\n",
        "DEVICE = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 256\n",
        "NUM_CLASSES = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQMmKUEisW4W"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbNGhmMzQx2J"
      },
      "source": [
        "The following cells will download the IMDB movie review dataset (http://ai.stanford.edu/~amaas/data/sentiment/) for positive-negative sentiment classification in as CSV-formatted file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fJAjhh3VQx2J",
        "outputId": "2d572cb6-9e6a-4f23-90f2-3913ab39113b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-13 23:24:26--  https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch08/movie_data.csv.gz [following]\n",
            "--2024-12-13 23:24:26--  https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch08/movie_data.csv.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26521894 (25M) [application/octet-stream]\n",
            "Saving to: ‘movie_data.csv.gz’\n",
            "\n",
            "movie_data.csv.gz   100%[===================>]  25.29M  81.4MB/s    in 0.3s    \n",
            "\n",
            "2024-12-13 23:24:27 (81.4 MB/s) - ‘movie_data.csv.gz’ saved [26521894/26521894]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HFvf0dA6Qx2J"
      },
      "outputs": [],
      "source": [
        "!gunzip -f movie_data.csv.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqADCcsgQx2J"
      },
      "source": [
        "Check that the dataset looks okay:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UEF-AWYzQx2K",
        "outputId": "b6301b34-5ff9-40d1-9ff1-54da17cb50b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
              "1  OK... so... I really like Kris Kristofferson a...          0\n",
              "2  ***SPOILER*** Do not read this, if you think a...          0\n",
              "3  hi for all the people who have seen this wonde...          1\n",
              "4  I recently bought the DVD, forgetting just how...          0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-86d9228c-b0e8-4b9a-967f-d47a8ab770b3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hi for all the people who have seen this wonde...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I recently bought the DVD, forgetting just how...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86d9228c-b0e8-4b9a-967f-d47a8ab770b3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-86d9228c-b0e8-4b9a-967f-d47a8ab770b3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-86d9228c-b0e8-4b9a-967f-d47a8ab770b3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1ed2bdd9-fecb-4859-b095-f5fe32990c72\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1ed2bdd9-fecb-4859-b095-f5fe32990c72')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1ed2bdd9-fecb-4859-b095-f5fe32990c72 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"Every time I think about this film I feel physically ill. To read such a great book and later discover there's a film of it was a great feeling. Years later and imagine my joy at switching on the sci-fi channel and finding it starts in just 5mins!!! Up go the titles and then uggg. If just a couple of things had changed OK. Everything is changed. Numerous characters are removed entirely new rubbish ones are added. The main hero is shrunk and de-aged by about 30 years, and hilariously his girlfriend/wife is now his mother! Even the dog is reduced to sub-lassie capabilities. This is truly appalling cinema at its absolute worst. I would quite happily remove my own toenails with pliers rather than sit through another horrific viewing, and I urge anyone thinking of watching this - please don't. If you own a copy burn it now, right now and think how much better your life would have been had this celluloid insult never occurred.\",\n          \"As with all environmentally aware films from the 1970s SOYLENT GREEN has a rather cheesy view of what ecological meltdown is . Overpopulation means there`s too many people to feed ? I was under the impression that famines were caused by either war or failed economic policies . Stalin`s policy in the Soviet Union in the 1930s left millions dead because of famine and to this day the greatest man made tragedy was Mao`s rural policy in China which led over 30 million starvation deaths in the 1950s . And let`s not forget the great famines in the horn of Africa in the 1980s and 90s which were to do with conflicts not overpopulation . You might like to also consider that two of the most heavily populated areas on Earth , Hong Kong and Macau , have never suffered a famine in modern times . Likewise the expansion of shanty towns around cities as seen here isn`t strictly down to overpopulation - it`s down to economic factors where people flock to cities to find better paid work than in the countryside ( It`s a symptom of industrial progress - not of too many births ) so the image of the streets of New York city being too congested to walk through and of having people sleep in stairwells is somewhat laughable<br /><br />But don`t be fooled into thinking SOYLENT GREEN is a pile of corny tree hugging crap because I consider this to be the best ecological film of the<br /><br />70s . It plays on the contempary audience`s knowledge of the world where Sol and Thorn are beside themselves with joy at finding fruit , brandy and fresh meat . Thorn gasps in amazement at having ice in his whisky , puffs on a cigarette and delivers the classic line \\\" If I could afford it I`d smoke two , maybe three of these a day \\\" . But it`s the visage of the euthanasia chamber that`s memorable as Thorn gazes at the images of wild animals , flowers , running water and snow covered mountains , a world Thorn`s generation has never known . This is a very haunting scene which makes SOYLENT GREEN a very memorable film , combined with the fact it features the final screen appearance of Edward G Robinson as the wise old Jew Sol Roth\",\n          \"Yah. I know. It has the name \\\"Sinatra\\\" in the title, so how bad can it be? Well, it's bad, trust me! I rented this thinking it was some movie I missed in the theaters. It's not. It's some garbage \\\"movie\\\" made by the folks at Showtime (cable station). Geez, these cable stations make a few bucks they think they can make whatever garbage movies they want! It's not good. I am as big a Sinatra fan as any sane man, but this movie was just dumb. Boring. Dull. Unfunny. Uninteresting. The only redeeming quality is that (assuming they did stick to the facts) you do learn about what happened to the captors of Frank Jr. Otherwise it's just a stupid film.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df = pd.read_csv('movie_data.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "-lrsJbNSQx2K"
      },
      "source": [
        "df.columns = ['TEXT_COLUMN_NAME', 'LABEL_COLUMN_NAME']\n",
        "df.to_csv('movie_data.csv', index=None)\n",
        "\n",
        "df = pd.read_csv('movie_data.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jqAeQnY_Qx2K"
      },
      "outputs": [],
      "source": [
        "del df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bZ_O-uoQx2K"
      },
      "source": [
        "## Prepare Dataset with Torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Zg2oOYE7Qx2K"
      },
      "outputs": [],
      "source": [
        "# !conda install spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYUybtFdQx2K"
      },
      "source": [
        "Download English vocabulary via:\n",
        "    \n",
        "- `python -m spacy download en_core_web_sm`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GnH64XvsV8n"
      },
      "source": [
        "Define the Label and Text field formatters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7yAeS90-Qx2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c59f0717-f7a4-4382-dc73-29803e1724c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 9\n"
          ]
        }
      ],
      "source": [
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
        "\n",
        "# Example dataset (replace with your dataset's text data)\n",
        "train_data = [\"I love this movie\", \"This is a bad movie\"]\n",
        "\n",
        "# Vocabulary\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkXVR7a4Qx2K"
      },
      "source": [
        "Process the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5t5RF8tzQx2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e044c6f-0590-4bcb-adde-6e3b2f2276fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 50000\n",
            "First item: (tensor([    11,   5681,      3,      1,   2123,   3956,  28450,     24,   4741,\n",
            "          1517,     23,   1097,      7,      1,  21396,   1557,      6,   6637,\n",
            "           724,      3,  14901,      3,   8576,      2,     27,      1,  15546,\n",
            "           323,      3,   3564,      6,   2099,      3,     61,     16,   1885,\n",
            "            11,      1,   8712,      6,     48,    332,      4,     48,    596,\n",
            "          4615,  15649,      2,  35014,    161,    310,      3,      1,    733,\n",
            "           930,  26779,     24,   1418,  20166,     23,      3,     41,      9,\n",
            "             5,   1094,   1121,   1355,     14,     52,   2982,     11,   5944,\n",
            "            19,  52791,     11,    796,      2,   1626,      2,   5259,   3149,\n",
            "             4,   1609,      7,  22888,      3,   1064,      7,   3830,      1,\n",
            "           422,     18,     31,   1934,   1748,   2263,     24,   3603,   3233,\n",
            "            23,     18,      1,   1245,      6,    497,      5,    282,      2,\n",
            "             1,   5419,  11104,      4,     88,     28,   2505,     99,      3,\n",
            "            21,     18,      1,   1415,      6,      1,   5065,   1355,   1281,\n",
            "          8815,     24,    602,  12870,     23,     14,     16,     11,   2722,\n",
            "             6,      1,   3393,     11,      1,   1395,      8,     15,      3,\n",
            "            38,   1805,      1,   1758,      4,      5,   5043,      6,    658,\n",
            "             4,    298,      7,   1000,      1,    596,      2,    596,     11,\n",
            "         14901,      9,      5,     56,    255,     20,      3,     18,      1,\n",
            "           302,     73,      6,      5,    596,      6,      5,   3458,    161,\n",
            "           179,    251,     14,     16,   2320,     39,      5,   2991,   2123,\n",
            "           639,    407,     16,      5,   3075,      2,      1,    954,      4,\n",
            "           980,    235,    337,     72,   2344,      7,   1000,      1,    596,\n",
            "            19,     59,     79,   2025,    161,      2,    195,      3,      5,\n",
            "         46378,   1355,      4,   6173, 122218,     11,   5944,     16,    483,\n",
            "             7,  25490,     93,      1,   4369,    838,     16,   2320,      2,\n",
            "             1,    898,    277,      1,   3393,      6,    930,      4,      1,\n",
            "           242,    487,      6,   3956,     11,   4210,      3,     21,     45,\n",
            "             9,      5,    569,      6,      1,   1399,     11,      1,  12857,\n",
            "             2,     62,   2020,      9,   1627,      2,    427,     24,   3454,\n",
            "            23,     28,   1255]), tensor(1))\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Define custom dataset\n",
        "class MovieDataset(Dataset):\n",
        "    def __init__(self, csv_file, text_column, label_column, vocab=None):\n",
        "        import pandas as pd\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.text_column = text_column\n",
        "        self.label_column = label_column\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Build vocabulary if not provided\n",
        "        if vocab is None:\n",
        "            self.vocab = build_vocab_from_iterator(self._yield_tokens(), specials=[\"<unk>\"])\n",
        "            self.vocab.set_default_index(self.vocab[\"<unk>\"])\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "\n",
        "    def _yield_tokens(self):\n",
        "        for text in self.df[self.text_column]:\n",
        "            yield self.tokenizer(text)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.df.iloc[idx][self.text_column]\n",
        "        label = self.df.iloc[idx][self.label_column]\n",
        "        # Convert text to token indices and label to tensor\n",
        "        token_indices = [self.vocab[token] for token in self.tokenizer(text)]\n",
        "        return torch.tensor(token_indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# Create dataset\n",
        "csv_file = \"movie_data.csv\"  # Replace with your dataset path\n",
        "text_column = \"review\"  # Replace with your text column name\n",
        "label_column = \"sentiment\"  # Replace with your label column name\n",
        "\n",
        "dataset = MovieDataset(csv_file, text_column, label_column)\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(f\"First item: {dataset[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmgv3yRBQx2L"
      },
      "source": [
        "## Split Dataset into Train/Validation/Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J02Ez1EYQx2L"
      },
      "source": [
        "Split the dataset into training, validation, and test partitions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ_4jiHVnMxN",
        "outputId": "b3bb7190-1617-413b-c449-443db974dc18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Train: 40000\n",
            "Num Test: 10000\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# Define split sizes\n",
        "train_size = int(0.8 * len(dataset))  # 80% for training\n",
        "test_size = len(dataset) - train_size  # Remaining 20% for testing\n",
        "\n",
        "# Perform the split\n",
        "train_data, test_data = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "print(f'Num Train: {len(train_data)}')\n",
        "print(f'Num Test: {len(test_data)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sizes for train, validation, and test sets\n",
        "train_size = int(0.7 * len(dataset))  # 70% for training\n",
        "valid_size = int(0.1 * len(dataset))  # 10% for validation\n",
        "test_size = len(dataset) - train_size - valid_size  # Remaining 20% for testing\n",
        "\n",
        "# Perform the split\n",
        "train_data, valid_data, test_data = random_split(dataset, [train_size, valid_size, test_size])\n",
        "\n",
        "print(f'Num Train: {len(train_data)}')\n",
        "print(f'Num Validation: {len(valid_data)}')\n",
        "print(f'Num Test: {len(test_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oq8MkGVDS7kA",
        "outputId": "83fe8ee9-1bfc-4e13-81d4-de8c81f0168e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Train: 35000\n",
            "Num Validation: 5000\n",
            "Num Test: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MiNZT9-XQx2L",
        "outputId": "24f6f95e-e121-49fb-8e31-18a6fd03ea3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Train: 29750\n",
            "Num Validation: 5250\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# Split train_data into training and validation sets\n",
        "train_size = int(0.85 * len(train_data))  # 85% of current train_data\n",
        "valid_size = len(train_data) - train_size  # Remaining 15% for validation\n",
        "\n",
        "train_data, valid_data = random_split(train_data, [train_size, valid_size])\n",
        "\n",
        "print(f'Num Train: {len(train_data)}')\n",
        "print(f'Num Validation: {len(valid_data)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DtehvnURQx2L",
        "outputId": "21b99aa5-b59a-4f37-b617-4dad8031558d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: tensor([    5,    60,   720,    22,     3,  3537,    25,   126,    93,     7,\n",
            "          111,   179,  3138,  2143,  4677,     2,    10,   206,    28,   221,\n",
            "          558,     7,  9348,  4845,    46,    63,  1746,   133,  4677,    17,\n",
            "            5,    22,  1691,     3,    21,    10,     8,    15,    80,    59,\n",
            "          225,    79,   131,   775,   428,   896,   251,    46,   281,     8,\n",
            "           15,   275,    27,     5,   124, 11911,   693,     2,    17,     5,\n",
            "          693,     3,    10,     8,    15,    64, 10242, 16343,    46,  9601,\n",
            "         4521,     3,    21,    10,    57,  7988,     1,   359, 87806,    19,\n",
            "          116,     4,     1,   918,  1103,  9722,     2,     1,    22,  1226,\n",
            "           11,   157,    67,   306,     5,   946,   124,     3,   615,   323,\n",
            "            6,     1, 21781,     3,   121,     5,   596,    81,     4,   530,\n",
            "          164,    81,     9,  1963, 11911,     2,     9,     1,  2686,   259,\n",
            "           51,   165,     1,  8902,   732,    53,   254,     3,    21,     1,\n",
            "         1630,     9,   250,     3,    63,   162,   665,   706,     4, 10533,\n",
            "           39, 12181,     6,   752,  2116,    24,  3489,    11,     1,  3263,\n",
            "            6,     1, 22409,  9500,     6,     1,  5474,   521,     3,  8180,\n",
            "         4109,    23,     2,     1,   365,   985,     9,    47,     5,   231,\n",
            "          105,   365,     3,     1,   979,   613,     9,    47,     5,   231,\n",
            "          105,  1469,     3,     1,   979,   774,     5,   231,   105, 22845,\n",
            "            3,     1,  8125, 22333,  4690,   231,   105,  8125,     3,     4,\n",
            "            1, 31335, 22536,     5,   231,   105, 32353,     3,    21,    17,\n",
            "         1966,   148,     3,    38,    30,   189,  3098,     2,    45,     8,\n",
            "           15,     5, 15122,     6,    70,  1499,  5474,   521,     3,    41,\n",
            "           30,  2306,    63,   132,    79,     1,   521,    19,     5, 10242,\n",
            "        16343,   693,     6,     1,   175,   822,     3,    41,   527,  2376,\n",
            "            7,    33,     5,   231,    27,     1, 14138,   510,     2,   423,\n",
            "            3,    13,    22,   102,    32,    83,    80,   132,    79,    10,\n",
            "            9,     3,     4,     1,  3623, 28037,   619,     9,    40,  4509,\n",
            "            3,    21,    50,    25,   354,  9489,    91,   179,   106,     3,\n",
            "           13,    87,  1886,     7,    33,     5,  6408, 15342,  1214,     2])\n",
            "Label: 1\n"
          ]
        }
      ],
      "source": [
        "# Access the first data item in the Subset\n",
        "example_idx = train_data.indices[0]  # Get the original index\n",
        "example = train_data.dataset[example_idx]  # Fetch the original data\n",
        "\n",
        "# Print the content\n",
        "print(f\"Text: {example[0]}\")  # Token indices or raw text depending on your dataset\n",
        "print(f\"Label: {example[1]}\")  # Label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQfxBn5CQx2L"
      },
      "source": [
        "## Build Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-TBwKWPslPa"
      },
      "source": [
        "Build the vocabulary based on the top \"VOCABULARY_SIZE\" words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8uNrjdtn4A8",
        "outputId": "031e1ea5-0793-43fc-e783-80efa65aa192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-796992a65d73>:14: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  text = text.cpu().numpy().tostring().decode('utf-8', errors='ignore')  # Decode bytes to string\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 20000\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Tokenizer for text\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Function to yield tokens from the dataset (ensure the text is in raw string format)\n",
        "def yield_tokens(data):\n",
        "    for text, _ in data:  # Access text part of each example\n",
        "        # If the text is a tensor, convert it to a string\n",
        "        if isinstance(text, torch.Tensor):\n",
        "            # Check if tensor contains bytes (in case of pre-processed tokens or encoded data)\n",
        "            text = text.cpu().numpy().tostring().decode('utf-8', errors='ignore')  # Decode bytes to string\n",
        "        elif isinstance(text, str):  # If it's already a string, no need to decode\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected type for text: {type(text)}\")\n",
        "\n",
        "        # Tokenize the text\n",
        "        yield tokenizer(text)\n",
        "\n",
        "# Build text vocabulary\n",
        "VOCABULARY_SIZE = 20000  # Define max vocabulary size\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"<unk>\", \"<pad>\"], max_tokens=VOCABULARY_SIZE)\n",
        "vocab.set_default_index(vocab[\"<unk>\"])  # Handle out-of-vocabulary tokens\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract unique labels\n",
        "labels = set(label.item() for _, label in train_data)\n",
        "\n",
        "# Create label-to-index mapping\n",
        "label_vocab = {label: idx for idx, label in enumerate(sorted(labels))}\n",
        "print(f\"Number of classes: {len(label_vocab)}\")\n",
        "print(f\"Label mapping: {label_vocab}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xrWnbZ7UW2T",
        "outputId": "1754b7f1-d303-4fb1-a4e0-43428fbf3a8e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 2\n",
            "Label mapping: {0: 0, 1: 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_batch(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    texts = [torch.tensor([vocab[token] for token in tokenizer(text)], dtype=torch.long) for text in texts]\n",
        "    texts = pad_sequence(texts, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
        "    labels = torch.tensor([label_vocab[label.item()] for label in labels], dtype=torch.long)\n",
        "    return texts, labels"
      ],
      "metadata": {
        "id": "FoODI5O1UZG_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# Tokenizer for text\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Function to yield tokens from train_data\n",
        "def yield_tokens(data):\n",
        "    for text, _ in data:  # Access text part of each example\n",
        "        if isinstance(text, torch.Tensor):  # If the text is a tensor, convert it\n",
        "            text = text.cpu().numpy().tostring().decode('utf-8', errors='ignore')  # Decode to string\n",
        "        elif isinstance(text, str):  # If it's already a string, no need to decode\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected type for text: {type(text)}\")\n",
        "\n",
        "        yield tokenizer(text)\n",
        "\n",
        "# Build text vocabulary\n",
        "VOCABULARY_SIZE = 20000  # Define max vocabulary size\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"<unk>\", \"<pad>\"], max_tokens=VOCABULARY_SIZE)\n",
        "vocab.set_default_index(vocab[\"<unk>\"])  # Handle out-of-vocabulary tokens\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "\n",
        "# Build label vocabulary (assuming labels are integers)\n",
        "labels = set(label.item() for _, label in train_data)  # Extract unique labels\n",
        "label_vocab = {label: idx for idx, label in enumerate(sorted(labels))}\n",
        "\n",
        "print(f\"Number of classes: {len(label_vocab)}\")\n",
        "print(f\"Label mapping: {label_vocab}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_5Wm_pkVaeI",
        "outputId": "d72afc22-76c0-475b-bb85-5d6f9dddbf15"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-ce9a9f9f3337>:12: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  text = text.cpu().numpy().tostring().decode('utf-8', errors='ignore')  # Decode to string\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 20000\n",
            "Number of classes: 2\n",
            "Label mapping: {0: 0, 1: 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18Kc6S08Qx2L"
      },
      "source": [
        "- 20,002 not 20,000 because of the `<unk>` and `<pad>` tokens\n",
        "- PyTorch RNNs can deal with arbitrary lengths due to dynamic graphs, but padding is necessary for padding sequences to the same length in a given minibatch so we can store those in an array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FXHS9MvQx2M"
      },
      "source": [
        "**Look at most common words:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "cZFJ8cCyQx2M",
        "outputId": "c79f9b68-0e6e-4b50-9ccc-d268479f3e41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-51c26c1016aa>:10: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  text = text.cpu().numpy().tostring().decode('utf-8', errors='ignore')  # Decode to string\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common 20 tokens: [('\\x00\\x00\\x00\\x00\\x00\\x00\\x00', 140946), ('!', 54479), ('\\x00\\x00\\x00\\x00\\x00\\x00', 54244), (\"'\", 46807), ('(', 45492), (')', 44631), (',', 42267), ('.', 39362), ('?', 29782), ('\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00', 16654), ('\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00', 10429), ('\\x01\\x00\\x00\\x00\\x00\\x00\\x00', 6903), ('\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00', 6858), ('\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00', 6050), ('\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00', 5646), ('\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00', 5635), ('\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00', 4811), ('\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x00\\x00\\x00\\x00', 4436), ('\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x0f\\x00\\x00\\x00\\x00\\x00\\x00\\x00', 4327), ('\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00', 3784)]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Manually count the frequencies of tokens in the dataset\n",
        "token_frequencies = Counter()\n",
        "\n",
        "# Function to yield tokens from the dataset (this is already defined)\n",
        "def yield_tokens(data):\n",
        "    for text, _ in data:  # Access text part of each example\n",
        "        if isinstance(text, torch.Tensor):  # If the text is a tensor, convert it\n",
        "            text = text.cpu().numpy().tostring().decode('utf-8', errors='ignore')  # Decode to string\n",
        "        elif isinstance(text, str):  # If it's already a string, no need to decode\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected type for text: {type(text)}\")\n",
        "\n",
        "        # Tokenize and update frequency count\n",
        "        tokens = tokenizer(text)\n",
        "        token_frequencies.update(tokens)\n",
        "\n",
        "# Apply to the training data\n",
        "yield_tokens(train_data)\n",
        "\n",
        "# Get the most common 20 tokens\n",
        "most_common_tokens = token_frequencies.most_common(20)\n",
        "\n",
        "# Print the result\n",
        "print(f\"Most common 20 tokens: {most_common_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIUYwzc_Qx2M"
      },
      "source": [
        "**Tokens corresponding to the first 10 indices (0, 1, ..., 9):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "89Bq7vjdQx2M",
        "outputId": "25c77ed3-f76b-49c4-dfa2-5a9c9d0b3682",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', '<pad>', '\\x00\\x00\\x00\\x00\\x00\\x00\\x00', '!', '\\x00\\x00\\x00\\x00\\x00\\x00', \"'\", '(', ')', ',', '.']\n"
          ]
        }
      ],
      "source": [
        "# Print the first 10 tokens from the vocabulary (integer-to-string mapping)\n",
        "itos = vocab.get_itos()  # Get integer-to-string mapping\n",
        "print(itos[:10])  # Print first 10 tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4DdhH6KQx2M"
      },
      "source": [
        "**Converting a string to an integer:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "eSv1EalpQx2M"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    text_list, label_list = zip(*batch)\n",
        "\n",
        "    # Ensure that text is a string (handle if it's tensor or numpy array)\n",
        "    text_list = [\n",
        "        text if isinstance(text, str) else\n",
        "        text.numpy().tobytes().decode('utf-8', errors='ignore') if isinstance(text, torch.Tensor) else\n",
        "        text\n",
        "        for text in text_list\n",
        "    ]\n",
        "\n",
        "    # Tokenize and convert to indices using the vocabulary\n",
        "    text_list = [\n",
        "        torch.tensor([vocab[token] for token in tokenizer(text)], dtype=torch.long)\n",
        "        for text in text_list\n",
        "    ]\n",
        "\n",
        "    # Pad the sequences to ensure the batch is of equal length\n",
        "    text_list = pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
        "\n",
        "    # Convert labels to tensor\n",
        "    label_list = torch.tensor(label_list, dtype=torch.long)\n",
        "\n",
        "    return text_list, label_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpUBrhYKQx2N"
      },
      "source": [
        "**Class labels:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "wN1O87rOQx2N",
        "outputId": "bfc37720-b9db-4e7a-8a37-20f1df3356c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label to index mapping (stoi): {0: 0, 1: 1}\n"
          ]
        }
      ],
      "source": [
        "# Print the string-to-integer mapping for labels\n",
        "print(f\"Label to index mapping (stoi): {label_vocab}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzHzt1TWQx2N"
      },
      "source": [
        "**Class label count:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "dKFP9MkNQx2N",
        "outputId": "49d0773e-ae4a-47b9-f7da-d4cc0bd9816e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label frequencies: Counter({0: 14948, 1: 14802})\n",
            "Most common 20 labels: [(0, 14948), (1, 14802)]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Create a counter to track label frequencies\n",
        "label_frequencies = Counter()\n",
        "\n",
        "# Iterate over the dataset to count label occurrences\n",
        "for _, label in train_data:\n",
        "    label_frequencies[label.item()] += 1  # Increment the count for each label\n",
        "\n",
        "# Print the frequencies of labels\n",
        "print(f\"Label frequencies: {label_frequencies}\")\n",
        "\n",
        "# Print the most common 20 labels and their frequencies\n",
        "most_common_labels = label_frequencies.most_common(20)\n",
        "print(f\"Most common 20 labels: {most_common_labels}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIQ_zfKLwjKm"
      },
      "source": [
        "## Define Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "i7JiHR1stHNF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "313f0ef5-f0d0-447e-956a-4468a8038c84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 - Text size: torch.Size([128, 254]), Labels size: torch.Size([128])\n",
            "Batch 2 - Text size: torch.Size([128, 243]), Labels size: torch.Size([128])\n",
            "Batch 3 - Text size: torch.Size([128, 264]), Labels size: torch.Size([128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-64-1cb4c9647179>:16: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  text.numpy().tostring().decode('utf-8', errors='ignore') if isinstance(text, torch.Tensor) else\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import torch\n",
        "\n",
        "# Tokenizer for text\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Create a custom collate function to pad sequences\n",
        "def collate_batch(batch):\n",
        "    text_list, label_list = zip(*batch)\n",
        "\n",
        "    # Ensure that text is a string (handle if it's tensor or numpy array)\n",
        "    text_list = [\n",
        "        text if isinstance(text, str) else\n",
        "        text.numpy().tostring().decode('utf-8', errors='ignore') if isinstance(text, torch.Tensor) else\n",
        "        text  # If text is already a string, pass as is\n",
        "        for text in text_list\n",
        "    ]\n",
        "\n",
        "    # Tokenize and convert to indices using the vocabulary\n",
        "    text_list = [\n",
        "        torch.tensor([vocab[token] for token in tokenizer(text)], dtype=torch.long)\n",
        "        for text in text_list\n",
        "    ]\n",
        "\n",
        "    # Pad the sequences to ensure the batch is of equal length\n",
        "    text_list = pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
        "\n",
        "    # Convert labels to tensor\n",
        "    label_list = torch.tensor(label_list, dtype=torch.long)\n",
        "\n",
        "    return text_list, label_list\n",
        "\n",
        "# Create DataLoader objects for train, valid, and test datasets\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_batch, shuffle=True)\n",
        "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE, collate_fn=collate_batch, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=collate_batch, shuffle=False)\n",
        "\n",
        "# Check if the DataLoader works properly\n",
        "for batch_idx, (text, labels) in enumerate(train_loader):\n",
        "    print(f'Batch {batch_idx + 1} - Text size: {text.size()}, Labels size: {labels.size()}')\n",
        "    if batch_idx > 1:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0pT_dMRvicQ"
      },
      "source": [
        "Testing the iterators (note that the number of rows depends on the longest document in the respective batch):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8SP_FccutT0",
        "outputId": "7ce39847-6c39-4473-df11-0f2191361109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train\n",
            "Text matrix size: torch.Size([128, 242])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Valid:\n",
            "Text matrix size: torch.Size([128, 291])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Test:\n",
            "Text matrix size: torch.Size([128, 310])\n",
            "Target vector size: torch.Size([128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-64-1cb4c9647179>:16: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  text.numpy().tostring().decode('utf-8', errors='ignore') if isinstance(text, torch.Tensor) else\n"
          ]
        }
      ],
      "source": [
        "print('Train')\n",
        "for batch in train_loader:\n",
        "    text, labels = batch  # Unpack the tuple\n",
        "    print(f'Text matrix size: {text.size()}')  # Size of the text tensor\n",
        "    print(f'Target vector size: {labels.size()}')  # Size of the label tensor\n",
        "    break\n",
        "\n",
        "print('\\nValid:')\n",
        "for batch in valid_loader:\n",
        "    text, labels = batch  # Unpack the tuple\n",
        "    print(f'Text matrix size: {text.size()}')  # Size of the text tensor\n",
        "    print(f'Target vector size: {labels.size()}')  # Size of the label tensor\n",
        "    break\n",
        "\n",
        "print('\\nTest:')\n",
        "for batch in test_loader:\n",
        "    text, labels = batch  # Unpack the tuple\n",
        "    print(f'Text matrix size: {text.size()}')  # Size of the text tensor\n",
        "    print(f'Target vector size: {labels.size()}')  # Size of the label tensor\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_grdW3pxCzz"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "nQIUm5EjxFNa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)  # Embedding layer\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)  # LSTM layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)  # Fully connected layer\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)  # Shape: [batch_size, seq_len, embedding_dim]\n",
        "        output, (hidden, cell) = self.rnn(embedded)  # LSTM returns output and hidden states\n",
        "\n",
        "        # Use only the final hidden state (the last time step)\n",
        "        hidden = hidden[-1]  # Shape: [batch_size, hidden_dim]\n",
        "\n",
        "        logits = self.fc(hidden)  # Shape: [batch_size, output_dim]\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "Ik3NF3faxFmZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2aae16a-8ee4-4105-bd14-d623cf096eed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Model initialized and ready for training.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available and choose the device accordingly\n",
        "if torch.cuda.is_available():\n",
        "    # Use the first available GPU (device 0)\n",
        "    DEVICE = torch.device('cuda:0')  # You can also use 'cuda' to default to the first available GPU\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Now you can safely move the model to the chosen device\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "model = RNN(input_dim=len(vocab),\n",
        "            embedding_dim=EMBEDDING_DIM,\n",
        "            hidden_dim=HIDDEN_DIM,\n",
        "            output_dim=NUM_CLASSES)  # Use 1 for binary classification\n",
        "\n",
        "model = model.to(DEVICE)  # Move the model to the selected device (CPU/GPU)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "print(\"Model initialized and ready for training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv9Ny9di6VcI"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "T5t1Afn4xO11"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(model, data_loader, device):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        correct_pred, num_examples = 0, 0\n",
        "\n",
        "        for i, (features, targets) in enumerate(data_loader):\n",
        "\n",
        "            features = features.to(device)\n",
        "            targets = targets.float().to(device)\n",
        "\n",
        "            logits = model(features)\n",
        "            _, predicted_labels = torch.max(logits, 1)\n",
        "\n",
        "            num_examples += targets.size(0)\n",
        "            correct_pred += (predicted_labels == targets).sum()\n",
        "    return correct_pred.float()/num_examples * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "EABZM8Vo0ilB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)  # batch_first=True\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)  # Shape: [batch_size, seq_len, embedding_dim]\n",
        "        output, (hidden, cell) = self.rnn(embedded)  # Shape of output: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        # Use only the final hidden state (the last time step)\n",
        "        hidden = hidden[-1]  # Shape: [batch_size, hidden_dim] (use the last layer's hidden state)\n",
        "\n",
        "        logits = self.fc(hidden)  # Shape: [batch_size, output_dim]\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)  # Embedding layer\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)  # LSTM layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)  # Fully connected layer\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)  # Shape: [batch_size, seq_len, embedding_dim]\n",
        "        output, (hidden, cell) = self.rnn(embedded)  # LSTM returns output and hidden states\n",
        "\n",
        "        # Use only the final hidden state (the last time step)\n",
        "        hidden = hidden[-1]  # Shape: [batch_size, hidden_dim]\n",
        "\n",
        "        logits = self.fc(hidden)  # Shape: [batch_size, output_dim]\n",
        "        return logits"
      ],
      "metadata": {
        "id": "g-I71XBPcLwn"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " NUM_EPOCHS = 15  # Set the number of epochs for training\n",
        "\n",
        "# The rest of the code remains the same\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    for batch_idx, batch_data in enumerate(train_loader):\n",
        "        text, labels = batch_data\n",
        "        text = text.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(text)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        optimizer.zero_grad()  # Zero gradients\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update model parameters\n",
        "\n",
        "        # Logging every 50 batches\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(f'Epoch: {epoch + 1:03d}/{NUM_EPOCHS:03d} | '\n",
        "                  f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
        "                  f'Loss: {loss:.4f}')\n",
        "\n",
        "    # Print training and validation accuracy\n",
        "    with torch.no_grad():\n",
        "        train_acc = compute_accuracy(model, train_loader, DEVICE)\n",
        "        valid_acc = compute_accuracy(model, valid_loader, DEVICE)\n",
        "        print(f'Training accuracy: {train_acc:.2f}%\\n'\n",
        "              f'Validation accuracy: {valid_acc:.2f}%')\n",
        "\n",
        "    print(f'Time elapsed: {(time.time() - start_time) / 60:.2f} min')\n",
        "\n",
        "# Final evaluation on the test set\n",
        "test_acc = compute_accuracy(model, test_loader, DEVICE)\n",
        "print(f'Total Training Time: {(time.time() - start_time) / 60:.2f} min')\n",
        "print(f'Test accuracy: {test_acc:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cdek3SPdPp7",
        "outputId": "7acaae7e-c34c-41cc-e67c-d104d6be575e"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-64-1cb4c9647179>:16: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  text.numpy().tostring().decode('utf-8', errors='ignore') if isinstance(text, torch.Tensor) else\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/015 | Batch 000/233 | Loss: 0.6945\n",
            "Epoch: 001/015 | Batch 050/233 | Loss: 0.6871\n",
            "Epoch: 001/015 | Batch 100/233 | Loss: 0.6905\n",
            "Epoch: 001/015 | Batch 150/233 | Loss: 0.6895\n",
            "Epoch: 001/015 | Batch 200/233 | Loss: 0.6944\n",
            "Training accuracy: 50.64%\n",
            "Validation accuracy: 50.10%\n",
            "Time elapsed: 1.07 min\n",
            "Epoch: 002/015 | Batch 000/233 | Loss: 0.6977\n",
            "Epoch: 002/015 | Batch 050/233 | Loss: 0.6924\n",
            "Epoch: 002/015 | Batch 100/233 | Loss: 0.6965\n",
            "Epoch: 002/015 | Batch 150/233 | Loss: 0.6862\n",
            "Epoch: 002/015 | Batch 200/233 | Loss: 0.6871\n",
            "Training accuracy: 50.64%\n",
            "Validation accuracy: 50.23%\n",
            "Time elapsed: 2.08 min\n",
            "Epoch: 003/015 | Batch 000/233 | Loss: 0.6888\n",
            "Epoch: 003/015 | Batch 050/233 | Loss: 0.6906\n",
            "Epoch: 003/015 | Batch 100/233 | Loss: 0.6943\n",
            "Epoch: 003/015 | Batch 150/233 | Loss: 0.6895\n",
            "Epoch: 003/015 | Batch 200/233 | Loss: 0.6869\n",
            "Training accuracy: 49.98%\n",
            "Validation accuracy: 49.79%\n",
            "Time elapsed: 3.08 min\n",
            "Epoch: 004/015 | Batch 000/233 | Loss: 0.6883\n",
            "Epoch: 004/015 | Batch 050/233 | Loss: 0.6873\n",
            "Epoch: 004/015 | Batch 100/233 | Loss: 0.6826\n",
            "Epoch: 004/015 | Batch 150/233 | Loss: 0.6951\n",
            "Epoch: 004/015 | Batch 200/233 | Loss: 0.7022\n",
            "Training accuracy: 60.12%\n",
            "Validation accuracy: 54.10%\n",
            "Time elapsed: 4.05 min\n",
            "Epoch: 005/015 | Batch 000/233 | Loss: 0.6675\n",
            "Epoch: 005/015 | Batch 050/233 | Loss: 0.6638\n",
            "Epoch: 005/015 | Batch 100/233 | Loss: 0.6672\n",
            "Epoch: 005/015 | Batch 150/233 | Loss: 0.6688\n",
            "Epoch: 005/015 | Batch 200/233 | Loss: 0.6477\n",
            "Training accuracy: 66.57%\n",
            "Validation accuracy: 56.99%\n",
            "Time elapsed: 5.01 min\n",
            "Epoch: 006/015 | Batch 000/233 | Loss: 0.6438\n",
            "Epoch: 006/015 | Batch 050/233 | Loss: 0.6213\n",
            "Epoch: 006/015 | Batch 100/233 | Loss: 0.5795\n",
            "Epoch: 006/015 | Batch 150/233 | Loss: 0.6356\n",
            "Epoch: 006/015 | Batch 200/233 | Loss: 0.5970\n",
            "Training accuracy: 72.15%\n",
            "Validation accuracy: 58.61%\n",
            "Time elapsed: 6.00 min\n",
            "Epoch: 007/015 | Batch 000/233 | Loss: 0.6023\n",
            "Epoch: 007/015 | Batch 050/233 | Loss: 0.5926\n",
            "Epoch: 007/015 | Batch 100/233 | Loss: 0.5417\n",
            "Epoch: 007/015 | Batch 150/233 | Loss: 0.5206\n",
            "Epoch: 007/015 | Batch 200/233 | Loss: 0.5395\n",
            "Training accuracy: 78.05%\n",
            "Validation accuracy: 60.44%\n",
            "Time elapsed: 6.95 min\n",
            "Epoch: 008/015 | Batch 000/233 | Loss: 0.4589\n",
            "Epoch: 008/015 | Batch 050/233 | Loss: 0.5300\n",
            "Epoch: 008/015 | Batch 100/233 | Loss: 0.4463\n",
            "Epoch: 008/015 | Batch 150/233 | Loss: 0.5435\n",
            "Epoch: 008/015 | Batch 200/233 | Loss: 0.4996\n",
            "Training accuracy: 82.27%\n",
            "Validation accuracy: 60.74%\n",
            "Time elapsed: 7.95 min\n",
            "Epoch: 009/015 | Batch 000/233 | Loss: 0.3899\n",
            "Epoch: 009/015 | Batch 050/233 | Loss: 0.3445\n",
            "Epoch: 009/015 | Batch 100/233 | Loss: 0.4301\n",
            "Epoch: 009/015 | Batch 150/233 | Loss: 0.4590\n",
            "Epoch: 009/015 | Batch 200/233 | Loss: 0.3462\n",
            "Training accuracy: 84.89%\n",
            "Validation accuracy: 61.60%\n",
            "Time elapsed: 8.91 min\n",
            "Epoch: 010/015 | Batch 000/233 | Loss: 0.3360\n",
            "Epoch: 010/015 | Batch 050/233 | Loss: 0.3467\n",
            "Epoch: 010/015 | Batch 100/233 | Loss: 0.3712\n",
            "Epoch: 010/015 | Batch 150/233 | Loss: 0.4264\n",
            "Epoch: 010/015 | Batch 200/233 | Loss: 0.3735\n",
            "Training accuracy: 87.55%\n",
            "Validation accuracy: 61.43%\n",
            "Time elapsed: 9.91 min\n",
            "Epoch: 011/015 | Batch 000/233 | Loss: 0.2881\n",
            "Epoch: 011/015 | Batch 050/233 | Loss: 0.3264\n",
            "Epoch: 011/015 | Batch 100/233 | Loss: 0.3771\n",
            "Epoch: 011/015 | Batch 150/233 | Loss: 0.2538\n",
            "Epoch: 011/015 | Batch 200/233 | Loss: 0.3045\n",
            "Training accuracy: 89.26%\n",
            "Validation accuracy: 60.97%\n",
            "Time elapsed: 10.87 min\n",
            "Epoch: 012/015 | Batch 000/233 | Loss: 0.2586\n",
            "Epoch: 012/015 | Batch 050/233 | Loss: 0.2127\n",
            "Epoch: 012/015 | Batch 100/233 | Loss: 0.2899\n",
            "Epoch: 012/015 | Batch 150/233 | Loss: 0.2160\n",
            "Epoch: 012/015 | Batch 200/233 | Loss: 0.2130\n",
            "Training accuracy: 91.37%\n",
            "Validation accuracy: 60.74%\n",
            "Time elapsed: 11.89 min\n",
            "Epoch: 013/015 | Batch 000/233 | Loss: 0.2432\n",
            "Epoch: 013/015 | Batch 050/233 | Loss: 0.1890\n",
            "Epoch: 013/015 | Batch 100/233 | Loss: 0.2238\n",
            "Epoch: 013/015 | Batch 150/233 | Loss: 0.2748\n",
            "Epoch: 013/015 | Batch 200/233 | Loss: 0.2776\n",
            "Training accuracy: 92.54%\n",
            "Validation accuracy: 60.93%\n",
            "Time elapsed: 12.85 min\n",
            "Epoch: 014/015 | Batch 000/233 | Loss: 0.1834\n",
            "Epoch: 014/015 | Batch 050/233 | Loss: 0.2211\n",
            "Epoch: 014/015 | Batch 100/233 | Loss: 0.1163\n",
            "Epoch: 014/015 | Batch 150/233 | Loss: 0.2775\n",
            "Epoch: 014/015 | Batch 200/233 | Loss: 0.2704\n",
            "Training accuracy: 93.92%\n",
            "Validation accuracy: 60.80%\n",
            "Time elapsed: 13.82 min\n",
            "Epoch: 015/015 | Batch 000/233 | Loss: 0.1463\n",
            "Epoch: 015/015 | Batch 050/233 | Loss: 0.1673\n",
            "Epoch: 015/015 | Batch 100/233 | Loss: 0.1935\n",
            "Epoch: 015/015 | Batch 150/233 | Loss: 0.1216\n",
            "Epoch: 015/015 | Batch 200/233 | Loss: 0.1381\n",
            "Training accuracy: 94.70%\n",
            "Validation accuracy: 60.72%\n",
            "Time elapsed: 14.81 min\n",
            "Total Training Time: 14.94 min\n",
            "Test accuracy: 61.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "jt55pscgFdKZ",
        "outputId": "74426b95-9ec7-4417-84a7-518ca9aede70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Number of GPUs available: 1\n",
            "Device 0: Tesla T4\n",
            "Using device: cuda:0\n",
            "Tokenized sentence: ['this', 'is', 'such', 'an', 'awesome', 'movie', ',', 'i', 'really', 'love', 'it', '!']\n",
            "Indexed tokens: [0, 0, 0, 0, 0, 0, 8, 8048, 0, 0, 0, 3]\n",
            "Probability positive: 0.7749\n",
            "Tokenized sentence: ['i', 'really', 'hate', 'this', 'movie', '.', 'it', 'is', 'really', 'bad', 'and', 'sucks', '!']\n",
            "Indexed tokens: [8048, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 3]\n",
            "Probability negative: 0.3285\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import torch\n",
        "\n",
        "# Assuming vocab is defined properly (replace this with your vocab creation)\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "def predict_sentiment(model, sentence, vocab, device):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the sentence using spaCy\n",
        "    tokenized = [tok.text.lower() for tok in nlp.tokenizer(sentence)]  # Lowercased tokens for consistency\n",
        "    print(f\"Tokenized sentence: {tokenized}\")  # Debug print to check tokenization\n",
        "\n",
        "    # Convert tokens to indices using the vocabulary\n",
        "    indexed = [vocab[token] if token in vocab else vocab['<unk>'] for token in tokenized]\n",
        "    print(f\"Indexed tokens: {indexed}\")  # Debug print to check the indexing\n",
        "\n",
        "    if len(indexed) == 0:\n",
        "        raise ValueError(\"The tokenized sentence is empty or contains only unknown tokens.\")\n",
        "\n",
        "    # Create a tensor from the indices and move it to the device (CPU or GPU)\n",
        "    tensor = torch.LongTensor(indexed).to(device)  # Shape: [seq_len]\n",
        "    tensor = tensor.unsqueeze(0)  # Shape: [1, seq_len] for batch size 1\n",
        "\n",
        "    # Pass the tensor through the model\n",
        "    logits = model(tensor)\n",
        "\n",
        "    # Get the probabilities using softmax\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "    return probabilities[0][1].item()  # Probability of the positive class (index 1)\n",
        "\n",
        "# Check CUDA availability and devices\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "\n",
        "# Dynamically set the device (GPU if available, otherwise CPU)\n",
        "try:\n",
        "    DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error in setting device: {e}\")\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "    print(f\"Using CPU instead.\")\n",
        "\n",
        "# Example Usage\n",
        "sentence = \"This is such an awesome movie, I really love it!\"\n",
        "print(f\"Probability positive: {predict_sentiment(model, sentence, vocab, DEVICE):.4f}\")\n",
        "\n",
        "# Calculate probability of negative sentiment\n",
        "negative_prob = 1 - predict_sentiment(model, \"I really hate this movie. It is really bad and sucks!\", vocab, DEVICE)\n",
        "\n",
        "# Print the probability of negative sentiment\n",
        "print(f'Probability negative: {negative_prob:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "j-8aUxZwQx2O",
        "outputId": "3087aa36-0486-4918-9e57-24cf8bc17dee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sentence: ['i', 'really', 'hate', 'this', 'movie', '.', 'it', 'is', 'really', 'bad', 'and', 'sucks', '!']\n",
            "Indexed tokens: [8048, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 3]\n",
            "Probability negative: 0.3285\n"
          ]
        }
      ],
      "source": [
        "# Calculate probability of negative sentiment\n",
        "negative_prob = 1 - predict_sentiment(model, \"I really hate this movie. It is really bad and sucks!\", vocab, DEVICE)\n",
        "\n",
        "# Print the probability of negative sentiment\n",
        "print(f'Probability negative: {negative_prob:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install watermark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLtVR1Dpg_lS",
        "outputId": "a9a41994-0bbc-46f2-e3a8-53402f1103a7"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting watermark\n",
            "  Downloading watermark-2.5.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: ipython>=6.0 in /usr/local/lib/python3.10/dist-packages (from watermark) (7.34.0)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from watermark) (8.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from watermark) (75.1.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->watermark) (3.21.0)\n",
            "Collecting jedi>=0.16 (from ipython>=6.0->watermark)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.0->watermark) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.0->watermark) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.0->watermark) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.0->watermark) (0.2.13)\n",
            "Downloading watermark-2.5.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, watermark\n",
            "Successfully installed jedi-0.19.2 watermark-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext watermark"
      ],
      "metadata": {
        "id": "v1n36y3AhCP_"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "7lRusB3dF80X",
        "outputId": "7dbfacac-330c-402d-8507-27baac609751",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pandas   : 2.2.2\n",
            "torch    : 2.0.1\n",
            "torchtext: 0.15.2\n",
            "spacy    : 3.7.5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%watermark -iv"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}